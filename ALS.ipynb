{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: Movie Recommender [-h] [--path [PATH]]\n",
      "                         [--movies_filename [MOVIES_FILENAME]]\n",
      "                         [--ratings_filename [RATINGS_FILENAME]]\n",
      "                         [--movie_name [MOVIE_NAME]] [--top_n TOP_N]\n",
      "Movie Recommender: error: unrecognized arguments: --f=/home/harsh-raj/.local/share/jupyter/runtime/kernel-v2-168069CL3M1I5ZC08.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsh-raj/.local/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# spark imports\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, lower\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "\n",
    "class AlsRecommender:\n",
    "    \"\"\"\n",
    "    This a collaborative filtering recommender with Alternating Least Square\n",
    "    Matrix Factorization, which is implemented by Spark\n",
    "    \"\"\"\n",
    "    def __init__(self, spark_session, path_movies, path_ratings):\n",
    "        self.spark = spark_session\n",
    "        self.sc = spark_session.sparkContext\n",
    "        self.moviesDF = self._load_file(path_movies) \\\n",
    "            .select(['tvId', 'name'])\n",
    "        self.ratingsDF = self._load_file(path_ratings) \\\n",
    "            .select(['userId', 'tvId', 'rating'])\n",
    "        self.model = ALS(\n",
    "            userCol='userId',\n",
    "            itemCol='tvId',\n",
    "            ratingCol='rating',\n",
    "            coldStartStrategy=\"drop\")\n",
    "\n",
    "    def _load_file(self, filepath):\n",
    "        \"\"\"\n",
    "        load csv file into memory as spark DF\n",
    "        \"\"\"\n",
    "        return self.spark.read.load(filepath, format='csv',\n",
    "                                    header=True, inferSchema=True)\n",
    "\n",
    "    def tune_model(self, maxIter, regParams, ranks, split_ratio=(6, 2, 2)):\n",
    "        \"\"\"\n",
    "        Hyperparameter tuning for ALS model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        maxIter: int, max number of learning iterations\n",
    "\n",
    "        regParams: list of float, regularization parameter\n",
    "\n",
    "        ranks: list of float, number of latent factors\n",
    "\n",
    "        split_ratio: tuple, (train, validation, test)\n",
    "        \"\"\"\n",
    "        # split data\n",
    "        train, val, test = self.ratingsDF.randomSplit(split_ratio)\n",
    "        # holdout tuning\n",
    "        self.model = tune_ALS(self.model, train, val,\n",
    "                              maxIter, regParams, ranks)\n",
    "        # test model\n",
    "        predictions = self.model.transform(test)\n",
    "        evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "                                        labelCol=\"rating\",\n",
    "                                        predictionCol=\"prediction\")\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "        print('The out-of-sample RMSE of the best tuned model is:', rmse)\n",
    "        # clean up\n",
    "        del train, val, test, predictions, evaluator\n",
    "        gc.collect()\n",
    "\n",
    "    def set_model_params(self, maxIter, regParam, rank):\n",
    "        \"\"\"\n",
    "        set model params for pyspark.ml.recommendation.ALS\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        maxIter: int, max number of learning iterations\n",
    "\n",
    "        regParams: float, regularization parameter\n",
    "\n",
    "        ranks: float, number of latent factors\n",
    "        \"\"\"\n",
    "        self.model = self.model \\\n",
    "            .setMaxIter(maxIter) \\\n",
    "            .setRank(rank) \\\n",
    "            .setRegParam(regParam)\n",
    "\n",
    "    def _regex_matching(self, fav_movie):\n",
    "        \"\"\"\n",
    "        return the closest matches via SQL regex.\n",
    "        If no match found, return None\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        fav_movie: str, name of user input movie\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        list of indices of the matching movies\n",
    "        \"\"\"\n",
    "        print('You have input movie:', fav_movie)\n",
    "        matchesDF = self.moviesDF \\\n",
    "            .filter(\n",
    "                lower(\n",
    "                    col('title')\n",
    "                ).like('%{}%'.format(fav_movie.lower()))\n",
    "            ) \\\n",
    "            .select('tvId', 'title')\n",
    "        if not len(matchesDF.take(1)):\n",
    "            print('Oops! No match is found')\n",
    "        else:\n",
    "            movieIds = matchesDF.rdd.map(lambda r: r[0]).collect()\n",
    "            titles = matchesDF.rdd.map(lambda r: r[1]).collect()\n",
    "            print('Found possible matches in our database: '\n",
    "                  '{0}\\n'.format([x for x in titles]))\n",
    "            return movieIds\n",
    "\n",
    "    def _append_ratings(self, userId, movieIds):\n",
    "        \"\"\"\n",
    "        append a user's movie ratings to ratingsDF\n",
    "\n",
    "        Parameter\n",
    "        ---------\n",
    "        userId: int, userId of a user\n",
    "\n",
    "        movieIds: int, movieIds of user's favorite movies\n",
    "        \"\"\"\n",
    "        # create new user rdd\n",
    "        user_rdd = self.sc.parallelize(\n",
    "            [(userId, tvId, 5.0) for tvId in movieIds])\n",
    "        # transform to user rows\n",
    "        user_rows = user_rdd.map(\n",
    "            lambda x: Row(\n",
    "                userId=int(x[0]),\n",
    "                tvId=int(x[1]),\n",
    "                rating=float(x[2])\n",
    "            )\n",
    "        )\n",
    "        # transform rows to spark DF\n",
    "        userDF = self.spark.createDataFrame(user_rows) \\\n",
    "            .select(self.ratingsDF.columns)\n",
    "        # append to ratingsDF\n",
    "        self.ratingsDF = self.ratingsDF.union(userDF)\n",
    "\n",
    "    def _create_inference_data(self, userId, movieIds):\n",
    "        \"\"\"\n",
    "        create a user with all movies except ones were rated for inferencing\n",
    "        \"\"\"\n",
    "        # filter movies\n",
    "        other_movieIds = self.moviesDF \\\n",
    "            .filter(~col('tvId').isin(movieIds)) \\\n",
    "            .select(['tvId']) \\\n",
    "            .rdd.map(lambda r: r[0]) \\\n",
    "            .collect()\n",
    "        # create inference rdd\n",
    "        inferenceRDD = self.sc.parallelize(\n",
    "            [(userId, tvId) for tvId in other_movieIds]\n",
    "        ).map(\n",
    "            lambda x: Row(\n",
    "                userId=int(x[0]),\n",
    "                tvId=int(x[1]),\n",
    "            )\n",
    "        )\n",
    "        # transform to inference DF\n",
    "        inferenceDF = self.spark.createDataFrame(inferenceRDD) \\\n",
    "            .select(['userId', 'tvId'])\n",
    "        return inferenceDF\n",
    "\n",
    "    def _inference(self, model, fav_movie, n_recommendations):\n",
    "        \"\"\"\n",
    "        return top n movie recommendations based on user's input movie\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model: spark ALS model\n",
    "\n",
    "        fav_movie: str, name of user input movie\n",
    "\n",
    "        n_recommendations: int, top n recommendations\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        list of top n similar movie recommendations\n",
    "        \"\"\"\n",
    "        # create a userId\n",
    "        userId = self.ratingsDF.agg({\"userId\": \"max\"}).collect()[0][0] + 1\n",
    "        # get movieIds of favorite movies\n",
    "        movieIds = self._regex_matching(fav_movie)\n",
    "        # append new user with his/her ratings into data\n",
    "        self._append_ratings(userId, movieIds)\n",
    "        # matrix factorization\n",
    "        model = model.fit(self.ratingsDF)\n",
    "        # get data for inferencing\n",
    "        inferenceDF = self._create_inference_data(userId, movieIds)\n",
    "        # make inference\n",
    "        return model.transform(inferenceDF) \\\n",
    "            .select(['tvId', 'prediction']) \\\n",
    "            .orderBy('prediction', ascending=False) \\\n",
    "            .rdd.map(lambda r: (r[0], r[1])) \\\n",
    "            .take(n_recommendations)\n",
    "\n",
    "    def make_recommendations(self, fav_movie, n_recommendations):\n",
    "        \"\"\"\n",
    "        make top n movie recommendations\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        fav_movie: str, name of user input movie\n",
    "\n",
    "        n_recommendations: int, top n recommendations\n",
    "        \"\"\"\n",
    "        # make inference and get raw recommendations\n",
    "        print('Recommendation system start to make inference ...')\n",
    "        t0 = time.time()\n",
    "        raw_recommends = \\\n",
    "            self._inference(self.model, fav_movie, n_recommendations)\n",
    "        movieIds = [r[0] for r in raw_recommends]\n",
    "        scores = [r[1] for r in raw_recommends]\n",
    "        print('It took my system {:.2f}s to make inference \\n\\\n",
    "              '.format(time.time() - t0))\n",
    "        # get movie titles\n",
    "        movie_titles = self.moviesDF \\\n",
    "            .filter(col('tvId').isin(movieIds)) \\\n",
    "            .select('title') \\\n",
    "            .rdd.map(lambda r: r[0]) \\\n",
    "            .collect()\n",
    "        # print recommendations\n",
    "        print('Recommendations for {}:'.format(fav_movie))\n",
    "        for i in range(len(movie_titles)):\n",
    "            print('{0}: {1}, with rating '\n",
    "                  'of {2}'.format(i+1, movie_titles[i], scores[i]))\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    \"\"\"\n",
    "    data object make loading raw files easier\n",
    "    \"\"\"\n",
    "    def __init__(self, spark_session, filepath):\n",
    "        \"\"\"\n",
    "        spark dataset constructor\n",
    "        \"\"\"\n",
    "        self.spark = spark_session\n",
    "        self.sc = spark_session.sparkContext\n",
    "        self.filepath = filepath\n",
    "        # build spark data object\n",
    "        self.RDD = self.load_file_as_RDD(self.filepath)\n",
    "        self.DF = self.load_file_as_DF(self.filepath)\n",
    "\n",
    "    def load_file_as_RDD(self, filepath):\n",
    "        ratings_RDD = self.sc.textFile(filepath)\n",
    "        header = ratings_RDD.take(1)[0]\n",
    "        return ratings_RDD \\\n",
    "            .filter(lambda line: line != header) \\\n",
    "            .map(lambda line: line.split(\",\")) \\\n",
    "            .map(lambda tokens: (int(tokens[0]), int(tokens[1]), float(tokens[2]))) # noqa\n",
    "\n",
    "    def load_file_as_DF(self, filepath):\n",
    "        ratings_RDD = self.load_file_as_rdd(filepath)\n",
    "        ratingsRDD = ratings_RDD.map(lambda tokens: Row(\n",
    "            userId=int(tokens[0]), tvId=int(tokens[1]), rating=float(tokens[2]))) # noqa\n",
    "        return self.spark.createDataFrame(ratingsRDD)\n",
    "\n",
    "\n",
    "def tune_ALS(model, train_data, validation_data, maxIter, regParams, ranks):\n",
    "    \"\"\"\n",
    "    grid search function to select the best model based on RMSE of\n",
    "    validation data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: spark ML model, ALS\n",
    "\n",
    "    train_data: spark DF with columns ['userId', 'tvId', 'rating']\n",
    "\n",
    "    validation_data: spark DF with columns ['userId', 'tvId', 'rating']\n",
    "\n",
    "    maxIter: int, max number of learning iterations\n",
    "\n",
    "    regParams: list of float, one dimension of hyper-param tuning grid\n",
    "\n",
    "    ranks: list of float, one dimension of hyper-param tuning grid\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    The best fitted ALS model with lowest RMSE score on validation data\n",
    "    \"\"\"\n",
    "    # initial\n",
    "    min_error = float('inf')\n",
    "    best_rank = -1\n",
    "    best_regularization = 0\n",
    "    best_model = None\n",
    "    for rank in ranks:\n",
    "        for reg in regParams:\n",
    "            # get ALS model\n",
    "            als = model.setMaxIter(maxIter).setRank(rank).setRegParam(reg)\n",
    "            # train ALS model\n",
    "            model = als.fit(train_data)\n",
    "            # evaluate the model by computing the RMSE on the validation data\n",
    "            predictions = model.transform(validation_data)\n",
    "            evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "                                            labelCol=\"rating\",\n",
    "                                            predictionCol=\"prediction\")\n",
    "            rmse = evaluator.evaluate(predictions)\n",
    "            print('{} latent factors and regularization = {}: '\n",
    "                  'validation RMSE is {}'.format(rank, reg, rmse))\n",
    "            if rmse < min_error:\n",
    "                min_error = rmse\n",
    "                best_rank = rank\n",
    "                best_regularization = reg\n",
    "                best_model = model\n",
    "    print('\\nThe best model has {} latent factors and '\n",
    "          'regularization = {}'.format(best_rank, best_regularization))\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        prog=\"Movie Recommender\",\n",
    "        description=\"Run ALS Movie Recommender\")\n",
    "    parser.add_argument('--path', nargs='?', default='../data/MovieLens',\n",
    "                        help='input data path')\n",
    "    parser.add_argument('--movies_filename', nargs='?', default='movies.csv',\n",
    "                        help='provide movies filename')\n",
    "    parser.add_argument('--ratings_filename', nargs='?', default='ratings.csv',\n",
    "                        help='provide ratings filename')\n",
    "    parser.add_argument('--movie_name', nargs='?', default='',\n",
    "                        help='provide your favoriate movie name')\n",
    "    parser.add_argument('--top_n', type=int, default=10,\n",
    "                        help='top n movie recommendations')\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # get args\n",
    "    args = parse_args()\n",
    "    data_path = args.path\n",
    "    movies_filename = args.movies_filename\n",
    "    ratings_filename = args.ratings_filename\n",
    "    movie_name = args.movie_name\n",
    "    top_n = args.top_n\n",
    "    # initial spark\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"movie recommender\") \\\n",
    "        .getOrCreate()\n",
    "    # initial recommender system\n",
    "    recommender = AlsRecommender(\n",
    "        spark,\n",
    "        os.path.join(data_path, movies_filename),\n",
    "        os.path.join(data_path, ratings_filename))\n",
    "    # set params\n",
    "    recommender.set_model_params(10, 0.05, 20)\n",
    "    # make recommendations\n",
    "    recommender.make_recommendations(movie_name, top_n)\n",
    "    # stop\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
